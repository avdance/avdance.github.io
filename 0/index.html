<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="icon" type="image/png" sizes="32x32" href="/images/avdance32_32.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/avdance16_16.ico"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"blog.avdancedu.com",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!0,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="1. Q: android camerax 如何使用 opengles ?A: 要使用 CameraX 和 OpenGL ES，你需要以下几个步骤：  添加 CameraX 依赖项到你的项目中。 创建一个 GLSurfaceView 来显示相机预览，并设置一个 GLSurfaceView.Renderer 来渲染 OpenGL 图形。 使用 CameraX 的 Preview 用例来获取相机预览数"><meta property="og:type" content="article"><meta property="og:title" content="音视跳动科技"><meta property="og:url" content="https://blog.avdancedu.com/0/index.html"><meta property="og:site_name" content="音视跳动科技"><meta property="og:description" content="1. Q: android camerax 如何使用 opengles ?A: 要使用 CameraX 和 OpenGL ES，你需要以下几个步骤：  添加 CameraX 依赖项到你的项目中。 创建一个 GLSurfaceView 来显示相机预览，并设置一个 GLSurfaceView.Renderer 来渲染 OpenGL 图形。 使用 CameraX 的 Preview 用例来获取相机预览数"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-05-07T11:56:46.858Z"><meta property="article:modified_time" content="2023-05-17T03:36:21.171Z"><meta property="article:author" content="音视跳动"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://blog.avdancedu.com/0/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title> | 音视跳动科技</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="音视跳动科技" type="application/atom+xml"></head><body itemscope itemtype="https://schema.org/WebPage"><div class="container"><div class="headband"></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div><header class="header" itemscope itemtype="https://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">音视跳动科技</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">传播最前沿的科技知识！</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div> <a href="https://github.com/avdance" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="https://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.avdancedu.com/0/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="image" content="/images/avdancelogo.jpg"><meta itemprop="name" content="音视跳动"><meta itemprop="description" content="传输最前沿的科技知识，学习音视频的圣地！ffmpeg, webrtc, H264, AAC"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="音视跳动科技"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"></h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-05-07 19:56:46" itemprop="dateCreated datePublished" datetime="2023-05-07T19:56:46+08:00">2023-05-07</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-05-17 11:36:21" itemprop="dateModified" datetime="2023-05-17T11:36:21+08:00">2023-05-17</time></span><span id="/0/" class="post-meta-item leancloud_visitors" data-flag-title="" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span class="leancloud-visitors-count"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span> <span class="post-meta-item-text">Valine：</span><a title="valine" href="/0/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/0/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>15k</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-Q-android-camerax-如何使用-opengles"><a href="#1-Q-android-camerax-如何使用-opengles" class="headerlink" title="1. Q: android camerax 如何使用 opengles ?"></a>1. Q: android camerax 如何使用 opengles ?</h2><p>A: 要使用 CameraX 和 OpenGL ES，你需要以下几个步骤：</p><ul><li>添加 CameraX 依赖项到你的项目中。</li><li>创建一个 GLSurfaceView 来显示相机预览，并设置一个 GLSurfaceView.Renderer 来渲染 OpenGL 图形。</li><li>使用 CameraX 的 Preview 用例来获取相机预览数据，并将其转换为 SurfaceTexture 对象。</li><li>将 SurfaceTexture 对象传递给 GLSurfaceView.Renderer 的 onSurfaceTextureAvailable() 方法，并在 onDrawFrame() 方法中使用 OpenGL ES 调用来渲染纹理多边形。</li><li>如果需要，你还可以使用 CameraX 的其他用例，如 ImageCapture, ImageAnalysis 或 VideoCapture，来实现拍照、图像分析或视频录制的功能。</li></ul><h2 id="2-Q-可以举个具体的例子吗？"><a href="#2-Q-可以举个具体的例子吗？" class="headerlink" title="2. Q: 可以举个具体的例子吗？"></a>2. Q: 可以举个具体的例子吗？</h2><p>A: 我可以给你一个使用 CameraX 和 OpenGL ES 的简单例子。假设你想在一个 Activity 中显示相机预览，并对预览图像应用一些 OpenGL ES 的滤镜效果。你可以按照以下步骤来实现：</p><ol><li>在你的 Activity 的布局文件中，添加一个 GLSurfaceView 控件，如下所示：</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">androidx.constraintlayout.widget.ConstraintLayout</span></span></span><br><span class="line"><span class="tag">    <span class="attr">xmlns:android</span>=<span class="string">&quot;http://schemas.android.com/apk/res/android&quot;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_width</span>=<span class="string">&quot;match_parent&quot;</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:layout_height</span>=<span class="string">&quot;match_parent&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">android.opengl.GLSurfaceView</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:id</span>=<span class="string">&quot;@+id/gl_surface_view&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_width</span>=<span class="string">&quot;match_parent&quot;</span></span></span><br><span class="line"><span class="tag">        <span class="attr">android:layout_height</span>=<span class="string">&quot;match_parent&quot;</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">androidx.constraintlayout.widget.ConstraintLayout</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>在你的 Activity 的 onCreate() 方法中，获取 GLSurfaceView 的引用，并设置一个自定义的 GLSurfaceView.Renderer 对象，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CameraActivity</span> : <span class="type">AppCompatActivity</span>() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">lateinit</span> <span class="keyword">var</span> glSurfaceView: GLSurfaceView</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">lateinit</span> <span class="keyword">var</span> cameraRenderer: CameraRenderer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onCreate</span><span class="params">(savedInstanceState: <span class="type">Bundle</span>?)</span></span> &#123;</span><br><span class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState)</span><br><span class="line">        setContentView(R.layout.activity_camera)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Get the reference to the GLSurfaceView</span></span><br><span class="line">        glSurfaceView = findViewById(R.id.gl_surface_view)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Set the EGL context to OpenGL ES 3.0</span></span><br><span class="line">        glSurfaceView.setEGLContextClientVersion(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create a custom renderer and set it to the GLSurfaceView</span></span><br><span class="line">        cameraRenderer = CameraRenderer(<span class="keyword">this</span>)</span><br><span class="line">        glSurfaceView.setRenderer(cameraRenderer)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>在你的自定义的 GLSurfaceView.Renderer 类中，实现 onSurfaceCreated(), onSurfaceChanged(), onDrawFrame(), onSurfaceTextureAvailable() 等方法，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CameraRenderer</span>(<span class="keyword">private</span> <span class="keyword">val</span> context: Context) : GLSurfaceView.Renderer, TextureView.SurfaceTextureListener &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">lateinit</span> <span class="keyword">var</span> cameraProviderFuture: ListenableFuture&lt;ProcessCameraProvider&gt;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">lateinit</span> <span class="keyword">var</span> preview: Preview</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">lateinit</span> <span class="keyword">var</span> surfaceTexture: SurfaceTexture</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> vertexShaderCode =</span><br><span class="line">        <span class="string">&quot;attribute vec4 vPosition;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;attribute vec2 inputTextureCoordinate;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;varying vec2 textureCoordinate;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;void main()&quot;</span> +</span><br><span class="line">                <span class="string">&quot;&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;gl_Position = vPosition;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;textureCoordinate = inputTextureCoordinate;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> fragmentShaderCode =</span><br><span class="line">        <span class="string">&quot;precision mediump float;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;varying vec2 textureCoordinate;\n&quot;</span> +</span><br><span class="line">                <span class="string">&quot;uniform sampler2D s_texture;\n&quot;</span> +</span><br><span class="line">                <span class="string">&quot;void main() &#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;  gl_FragColor = texture2D( s_texture, textureCoordinate );\n&quot;</span> +</span><br><span class="line">                <span class="string">&quot;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> vertexBuffer: FloatBuffer =</span><br><span class="line">        <span class="comment">// (# of coordinate values * 4 bytes per float)</span></span><br><span class="line">        ByteBuffer.allocateDirect(triangleCoords.size * <span class="number">4</span>).run &#123;</span><br><span class="line">            <span class="comment">// use the device hardware&#x27;s native byte order</span></span><br><span class="line">            order(ByteOrder.nativeOrder())</span><br><span class="line"></span><br><span class="line">            <span class="comment">// create a floating point buffer from the ByteBuffer</span></span><br><span class="line">            asFloatBuffer().apply &#123;</span><br><span class="line">                <span class="comment">// add the coordinates to the FloatBuffer</span></span><br><span class="line">                put(triangleCoords)</span><br><span class="line">                <span class="comment">// set the buffer to read the first coordinate</span></span><br><span class="line">                position(<span class="number">0</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> textureBuffer: FloatBuffer =</span><br><span class="line">        <span class="comment">// (# of coordinate values * 4 bytes per float)</span></span><br><span class="line">        ByteBuffer.allocateDirect(textureCoords.size * <span class="number">4</span>).run &#123;</span><br><span class="line">            <span class="comment">// use the device hardware&#x27;s native byte order</span></span><br><span class="line">            order(ByteOrder.nativeOrder())</span><br><span class="line"></span><br><span class="line">            <span class="comment">// create a floating point buffer from the ByteBuffer</span></span><br><span class="line">            asFloatBuffer().apply &#123;</span><br><span class="line">                <span class="comment">// add the coordinates to the FloatBuffer</span></span><br><span class="line">                put(textureCoords)</span><br><span class="line">                <span class="comment">// set the buffer to read the first coordinate</span></span><br><span class="line">                position(<span class="number">0</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> drawOrder = shortArrayOf(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>) <span class="comment">// order to draw vertices</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> drawListBuffer: ShortBuffer =</span><br><span class="line">        <span class="comment">// (# of coordinate values * 2 bytes per short)</span></span><br><span class="line">        ByteBuffer.allocateDirect(drawOrder.size * <span class="number">2</span>).run &#123;</span><br><span class="line">            order(ByteOrder.nativeOrder())</span><br><span class="line">            asShortBuffer().apply &#123;</span><br><span class="line">                put(drawOrder)</span><br><span class="line">                position(<span class="number">0</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> mProgram: <span class="built_in">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onSurfaceCreated</span><span class="params">(unused: <span class="type">GL10</span>, config: <span class="type">EGLConfig</span>)</span></span> &#123;</span><br><span class="line">        GLES30.glClearColor(<span class="number">0.0f</span>, <span class="number">0.0f</span>,</span><br></pre></td></tr></table></figure><ol start="4"><li>在 onSurfaceCreated() 方法中，编译和链接 OpenGL ES 着色器，并获取顶点位置和纹理坐标的句柄，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onSurfaceCreated</span><span class="params">(unused: <span class="type">GL10</span>, config: <span class="type">EGLConfig</span>)</span></span> &#123;</span><br><span class="line">    GLES30.glClearColor(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compile and link the vertex and fragment shaders</span></span><br><span class="line">    <span class="keyword">val</span> vertexShader: <span class="built_in">Int</span> = loadShader(GLES30.GL_VERTEX_SHADER, vertexShaderCode)</span><br><span class="line">    <span class="keyword">val</span> fragmentShader: <span class="built_in">Int</span> = loadShader(GLES30.GL_FRAGMENT_SHADER, fragmentShaderCode)</span><br><span class="line"></span><br><span class="line">    mProgram = GLES30.glCreateProgram().also &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// add the vertex shader to program</span></span><br><span class="line">        GLES30.glAttachShader(it, vertexShader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// add the fragment shader to program</span></span><br><span class="line">        GLES30.glAttachShader(it, fragmentShader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// creates OpenGL ES program executables</span></span><br><span class="line">        GLES30.glLinkProgram(it)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get the handles to the vertex position and texture coordinate attributes</span></span><br><span class="line">    <span class="keyword">val</span> positionHandle = GLES30.glGetAttribLocation(mProgram, <span class="string">&quot;vPosition&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> textureCoordHandle = GLES30.glGetAttribLocation(mProgram, <span class="string">&quot;inputTextureCoordinate&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Enable the attributes for drawing</span></span><br><span class="line">    GLES30.glEnableVertexAttribArray(positionHandle)</span><br><span class="line">    GLES30.glEnableVertexAttribArray(textureCoordHandle)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set the data for the attributes</span></span><br><span class="line">    GLES30.glVertexAttribPointer(</span><br><span class="line">        positionHandle,</span><br><span class="line">        COORDS_PER_VERTEX,</span><br><span class="line">        GLES30.GL_FLOAT,</span><br><span class="line">        <span class="literal">false</span>,</span><br><span class="line">        vertexStride,</span><br><span class="line">        vertexBuffer</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    GLES30.glVertexAttribPointer(</span><br><span class="line">        textureCoordHandle,</span><br><span class="line">        COORDS_PER_TEXTURE,</span><br><span class="line">        GLES30.GL_FLOAT,</span><br><span class="line">        <span class="literal">false</span>,</span><br><span class="line">        textureStride,</span><br><span class="line">        textureBuffer</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="5"><li>在 onSurfaceChanged() 方法中，设置视口大小，并初始化 CameraX 的 Preview 用例，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onSurfaceChanged</span><span class="params">(unused: <span class="type">GL10</span>, width: <span class="type">Int</span>, height: <span class="type">Int</span>)</span></span> &#123;</span><br><span class="line">    GLES30.glViewport(<span class="number">0</span>, <span class="number">0</span>, width, height)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize the CameraX Preview use case</span></span><br><span class="line">    preview = Preview.Builder()</span><br><span class="line">        .setTargetResolution(Size(width, height))</span><br><span class="line">        .build()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get a camera provider instance</span></span><br><span class="line">    cameraProviderFuture = ProcessCameraProvider.getInstance(context)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Bind the preview use case to a lifecycle owner (such as an activity) and a surface provider</span></span><br><span class="line">    cameraProviderFuture.addListener(Runnable &#123;</span><br><span class="line">        <span class="keyword">val</span> cameraProvider = cameraProviderFuture.<span class="keyword">get</span>()</span><br><span class="line">        <span class="keyword">val</span> cameraSelector = CameraSelector.Builder()</span><br><span class="line">            .requireLensFacing(CameraSelector.LENS_FACING_BACK)</span><br><span class="line">            .build()</span><br><span class="line">        cameraProvider.bindToLifecycle(context <span class="keyword">as</span> LifecycleOwner, cameraSelector, preview)</span><br><span class="line">        preview.setSurfaceProvider(<span class="keyword">this</span>)</span><br><span class="line">    &#125;, ContextCompat.getMainExecutor(context))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="6"><li>在 onDrawFrame() 方法中，绑定纹理到相机预览数据，并绘制多边形，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onDrawFrame</span><span class="params">(unused: <span class="type">GL10</span>)</span></span> &#123;</span><br><span class="line">    GLES30.glClear(GLES30.GL_COLOR_BUFFER_BIT)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the program object for rendering</span></span><br><span class="line">    GLES30.glUseProgram(mProgram)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Generate a texture and bind it to the SurfaceTexture object</span></span><br><span class="line">    <span class="keyword">val</span> textures = IntArray(<span class="number">1</span>)</span><br><span class="line">    GLES30.glGenTextures(<span class="number">1</span>, textures, <span class="number">0</span>)</span><br><span class="line">    surfaceTexture.attachToGLContext(textures[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Draw the polygon with the texture</span></span><br><span class="line">    GLES30.glDrawElements(</span><br><span class="line">        GLES30.GL_TRIANGLES, drawOrder.size,</span><br><span class="line">        GLES30.GL_UNSIGNED_SHORT, drawListBuffer</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update the SurfaceTexture object with the camera preview data</span></span><br><span class="line">    surfaceTexture.updateTexImage()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="7"><li>在 onSurfaceTextureAvailable() 方法中，获取相机预览数据的 SurfaceTexture 对象，并设置其默认缓冲区大小，如下所示：</li></ol><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onSurfaceTextureAvailable</span><span class="params">(surface: <span class="type">SurfaceTexture</span>?, width: <span class="type">Int</span>, height: <span class="type">Int</span>)</span></span> &#123;</span><br><span class="line">    surfaceTexture = surface!!</span><br><span class="line">    surfaceTexture.setDefaultBufferSize(width, height)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样，你就完成了一个使用 CameraX 和 OpenGL ES 的简单应用。你可以运行这个应用，并看到相机预览在 GLSurfaceView 中显示，并应用了 OpenGL ES 的默认滤镜效果。</p><h2 id="3-Q-在上述代码中，是如何将preview与-GLSurfaceView联系起来的？"><a href="#3-Q-在上述代码中，是如何将preview与-GLSurfaceView联系起来的？" class="headerlink" title="3. Q: 在上述代码中，是如何将preview与 GLSurfaceView联系起来的？"></a>3. Q: 在上述代码中，是如何将preview与 GLSurfaceView联系起来的？</h2><p>A: 在上述代码中，将 preview 与 GLSurfaceView 联系起来的关键是使用 preview.setSurfaceProvider(this) 方法，这个方法接受一个 SurfaceProvider 接口的实例，用于提供一个 Surface 对象给 preview 用例。在这个例子中，我们让自定义的 GLSurfaceView.Renderer 类实现了 TextureView.SurfaceTextureListener 接口，这个接口继承了 SurfaceProvider 接口，并重写了 onSurfaceTextureAvailable() 方法，用于获取相机预览数据的 SurfaceTexture 对象。然后，我们将这个对象传递给 OpenGL ES 的渲染器，用于绘制纹理多边形。这样，就实现了将 preview 与 GLSurfaceView 联系起来的功能。</p><h2 id="4-Q-能把相机获取图像后，数据流转到preview的完整过程详细描述一下吗？"><a href="#4-Q-能把相机获取图像后，数据流转到preview的完整过程详细描述一下吗？" class="headerlink" title="4. Q: 能把相机获取图像后，数据流转到preview的完整过程详细描述一下吗？"></a>4. Q: 能把相机获取图像后，数据流转到preview的完整过程详细描述一下吗？</h2><p>A: 相机获取图像后，数据流转到 preview 的完整过程如下：</p><ol><li>相机硬件通过 Camera2 API 获取图像数据，并将其封装为一个 Image 对象，这个对象包含了图像的元数据和像素数据。</li><li>CameraX 的 Preview 用例通过一个 ImageReader 对象获取 Image 对象，并将其转换为一个 Surface 对象，这个对象表示一个可绘制的表面，用于显示图像数据。</li><li>Preview 用例将 Surface 对象传递给 SurfaceProvider 接口的实例，这个接口定义了如何提供 Surface 对象给 Preview 用例。在这个例子中，我们使用 TextureView.SurfaceTextureListener 接口的实例作为 SurfaceProvider，这个接口继承了 SurfaceProvider 接口，并重写了 onSurfaceTextureAvailable() 方法，用于获取相机预览数据的 SurfaceTexture 对象。</li><li>SurfaceTexture 对象是一个特殊的 Surface 对象，它包含了一个 OpenGL ES 纹理对象，用于存储图像数据。我们将这个对象传递给 OpenGL ES 的渲染器，并在 onDrawFrame() 方法中使用 OpenGL ES 调用来渲染纹理多边形。</li><li>OpenGL ES 的渲染器将渲染结果输出到 GLSurfaceView 控件中，这个控件是一个 View 子类，用于显示 OpenGL ES 图形。这样，就完成了相机获取图像后，数据流转到 preview 的过程。</li></ol><h2 id="5-Q-这里有几个点我觉得挺奇怪。咱们来一项项讨论。第一个是preview-是camerax的第一个接收者吗？如果是的话，在xml中是不是要定义这个preview控件呢？"><a href="#5-Q-这里有几个点我觉得挺奇怪。咱们来一项项讨论。第一个是preview-是camerax的第一个接收者吗？如果是的话，在xml中是不是要定义这个preview控件呢？" class="headerlink" title="5. Q: 这里有几个点我觉得挺奇怪。咱们来一项项讨论。第一个是preview 是camerax的第一个接收者吗？如果是的话，在xml中是不是要定义这个preview控件呢？"></a>5. Q: 这里有几个点我觉得挺奇怪。咱们来一项项讨论。第一个是preview 是camerax的第一个接收者吗？如果是的话，在xml中是不是要定义这个preview控件呢？</h2><p>A: Preview 是 CameraX 的一个用例，它可以接收相机的图像数据，并将其显示在一个 Surface 对象上。Preview 不是一个控件，所以不需要在 xml 中定义。你只需要在代码中创建一个 Preview.Builder 对象，并设置一些参数，如目标分辨率、旋转角度等，然后调用 build() 方法来创建一个 Preview 对象。然后，你需要将这个 Preview 对象绑定到一个 LifecycleOwner 对象（如 Activity）和一个 SurfaceProvider 对象上，这样 Preview 就可以自动管理相机的生命周期，并将图像数据传递给 SurfaceProvider 提供的 Surface 对象。你可以使用任何实现了 SurfaceProvider 接口的对象作为 SurfaceProvider，如 TextureView、SurfaceView、GLSurfaceView 等。在这个例子中，我们使用了 GLSurfaceView 作为 SurfaceProvider，因为它可以方便地与 OpenGL ES 集成。</p><h2 id="6-Q-第二个问题"><a href="#6-Q-第二个问题" class="headerlink" title="6. Q: 第二个问题"></a>6. Q: 第二个问题</h2><p>那为什么在 <a target="_blank" rel="noopener" href="https://developer.android.com/codelabs/camerax-getting-started?hl=zh-cn#1%E4%B8%AD%E8%A6%81%E5%9C%A8xml%E4%B8%AD%E5%AE%9A%E4%B9%89">https://developer.android.com/codelabs/camerax-getting-started?hl=zh-cn#1中要在xml中定义</a> &lt;androidx.camera.view.PreviewView android:id&#x3D;”@+id&#x2F;viewFinder” android:layout_width&#x3D;”match_parent” android:layout_height&#x3D;”match_parent” &#x2F;&gt; 这个？</p><p>A: 这是因为在那个教程中，使用的是 androidx.camera.view.PreviewView 这个控件，它是一个专门为 CameraX 设计的控件，它实现了 SurfaceProvider 接口，并提供了一些额外的功能，如缩放、手势、裁剪等。你可以把它看作是一个封装了 CameraX 的 Preview 用例的控件，所以你需要在 xml 中定义它。但是，你也可以不使用 PreviewView，而是使用其他的 SurfaceProvider 实现，如 GLSurfaceView，这样你就不需要在 xml 中定义它，而是在代码中创建它，并设置一个自定义的渲染器。</p><h2 id="7-Q-ok-按照你的说法，Camerax获取到图像后，会将数据转交给preview，我们再为preview提供一下SurfaceProvider-这样就可以通过SurfaceProvider拿到图像数据，然后再通过GLSurfaceView和-Render就可以对数据使用opengles进行渲染，并最终将渲染的结果在屏幕上显示出来了是吧？"><a href="#7-Q-ok-按照你的说法，Camerax获取到图像后，会将数据转交给preview，我们再为preview提供一下SurfaceProvider-这样就可以通过SurfaceProvider拿到图像数据，然后再通过GLSurfaceView和-Render就可以对数据使用opengles进行渲染，并最终将渲染的结果在屏幕上显示出来了是吧？" class="headerlink" title="7. Q: ok, 按照你的说法，Camerax获取到图像后，会将数据转交给preview，我们再为preview提供一下SurfaceProvider, 这样就可以通过SurfaceProvider拿到图像数据，然后再通过GLSurfaceView和 Render就可以对数据使用opengles进行渲染，并最终将渲染的结果在屏幕上显示出来了是吧？"></a>7. Q: ok, 按照你的说法，Camerax获取到图像后，会将数据转交给preview，我们再为preview提供一下SurfaceProvider, 这样就可以通过SurfaceProvider拿到图像数据，然后再通过GLSurfaceView和 Render就可以对数据使用opengles进行渲染，并最终将渲染的结果在屏幕上显示出来了是吧？</h2><p>A: 是的，你的理解是正确的。CameraX 的 Preview 用例会将图像数据转交给 SurfaceProvider，然后你可以通过 SurfaceProvider 拿到图像数据，再通过 GLSurfaceView 和 Renderer 使用 OpenGL ES 进行渲染，并最终将渲染的结果在屏幕上显示出来。</p><h2 id="8-Q-OK-那如果想将处理后的图片保存起来，还可以通过-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-这种方式吗？应该不可以了吧？"><a href="#8-Q-OK-那如果想将处理后的图片保存起来，还可以通过-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-这种方式吗？应该不可以了吧？" class="headerlink" title="8. Q:OK, 那如果想将处理后的图片保存起来，还可以通过 cameraProvider.bindToLifecycle( this, cameraSelector, preview, imageCapture) 这种方式吗？应该不可以了吧？"></a>8. Q:OK, 那如果想将处理后的图片保存起来，还可以通过 cameraProvider.bindToLifecycle( this, cameraSelector, preview, imageCapture) 这种方式吗？应该不可以了吧？</h2><p>A: 如果你想将处理后的图片保存起来，你可以使用 CameraX 的 ImageCapture 用例，它可以让你拍摄并保存图片到存储中。你可以通过 cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageCapture) 这种方式将 ImageCapture 用例绑定到相同的相机和生命周期，这样你就可以同时使用 Preview 和 ImageCapture 的功能。但是，这样保存的图片是没有经过 OpenGL ES 处理的原始图片，如果你想保存处理后的图片，你需要另外一种方式。一种可能的方式是，在 Renderer 的 onDrawFrame() 方法中，使用 GLES30.glReadPixels() 方法来读取渲染后的像素数据，并将其转换为 Bitmap 对象，然后使用 MediaStore API 或其他方式来保存 Bitmap 对象到存储中。这样，你就可以保存处理后的图片了。</p><h2 id="9-Q-GLES30-glReadPixels-方法的作用是将GPU的数据拷贝到CPU中吗？"><a href="#9-Q-GLES30-glReadPixels-方法的作用是将GPU的数据拷贝到CPU中吗？" class="headerlink" title="9. Q: GLES30.glReadPixels() 方法的作用是将GPU的数据拷贝到CPU中吗？"></a>9. Q: GLES30.glReadPixels() 方法的作用是将GPU的数据拷贝到CPU中吗？</h2><p>A: 是的，GLES30.glReadPixels() 方法的作用是将 GPU 的帧缓冲区中的像素数据拷贝到 CPU 的内存中，这样你就可以对这些数据进行操作或保存。但是，这个方法的性能可能不是很好，因为它涉及到跨设备的数据传输，所以你应该尽量避免频繁地调用这个方法，或者只在需要保存图片的时候调用这个方法。</p><h2 id="10-Q-那通过-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-这种方式存储图片时，它底层是否也会将-GPU的数据拷贝到CPU再编过编码保存到磁盘中呢？"><a href="#10-Q-那通过-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-这种方式存储图片时，它底层是否也会将-GPU的数据拷贝到CPU再编过编码保存到磁盘中呢？" class="headerlink" title="10. Q: 那通过 cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageCapture) 这种方式存储图片时，它底层是否也会将 GPU的数据拷贝到CPU再编过编码保存到磁盘中呢？"></a>10. Q: 那通过 cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageCapture) 这种方式存储图片时，它底层是否也会将 GPU的数据拷贝到CPU再编过编码保存到磁盘中呢？</h2><p>A: 通过 cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageCapture) 这种方式存储图片时，它底层的实现可能会涉及到将 GPU 的数据拷贝到 CPU 再编码保存到磁盘中，但这个过程是由 CameraX 和 Camera2 API 来管理的，我们不需要关心具体的细节。我们只需要创建一个 ImageCapture 用例，并设置一些参数，如目标分辨率、闪光灯模式、捕获模式等¹，然后调用 takePicture() 方法来拍摄并保存图片²。我们还可以指定一个输出文件或者一个输出流来保存图片，或者使用一个 ImageCapture.OnImageCapturedCallback 来获取一个 ImageProxy 对象，用于访问图片的元数据和像素数据²。我们还可以指定一个 IO executor 来执行图片的保存任务，或者使用 CameraX 的默认 IO executor²。这样，我们就可以使用 CameraX 的 ImageCapture 用例来简化图片的存储过程。</p><p>源: 与必应的对话， 2023&#x2F;5&#x2F;7<br>(1) CameraX architecture | Android Developers. <a target="_blank" rel="noopener" href="https://developer.android.com/training/camerax/architecture">https://developer.android.com/training/camerax/architecture</a>.<br>(2) Getting Started with CameraX | Android Developers. <a target="_blank" rel="noopener" href="https://developer.android.com/codelabs/camerax-getting-started">https://developer.android.com/codelabs/camerax-getting-started</a>.<br>(3) Image capture | Android Developers. <a target="_blank" rel="noopener" href="https://developer.android.com/training/camerax/take-photo">https://developer.android.com/training/camerax/take-photo</a>.</p><h2 id="11-Q-我的意思是，如果涉及到保存图片，或保存视频，是否就绕不开从GPU中拷贝数据到CPU-然后编码，最后保存到磁盘中这样一个流程呢？"><a href="#11-Q-我的意思是，如果涉及到保存图片，或保存视频，是否就绕不开从GPU中拷贝数据到CPU-然后编码，最后保存到磁盘中这样一个流程呢？" class="headerlink" title="11. Q: 我的意思是，如果涉及到保存图片，或保存视频，是否就绕不开从GPU中拷贝数据到CPU,然后编码，最后保存到磁盘中这样一个流程呢？"></a>11. Q: 我的意思是，如果涉及到保存图片，或保存视频，是否就绕不开从GPU中拷贝数据到CPU,然后编码，最后保存到磁盘中这样一个流程呢？</h2><p>A: 我明白你的意思。一般来说，如果涉及到保存图片或视频，是需要从 GPU 中拷贝数据到 CPU，然后编码，最后保存到磁盘中的。但是，这个流程不一定是由我们来控制的，有时候是由系统或者第三方库来管理的。例如，如果我们使用 CameraX 的 ImageCapture 或 VideoCapture 用例来保存图片或视频，那么这个流程就是由 CameraX 和 Camera2 API 来管理的，我们不需要自己去拷贝数据或编码数据，只需要指定一个输出文件或流就可以了。另一种情况是，如果我们使用 OpenGL ES 来处理图片或视频，并想要保存处理后的结果，那么这个流程就需要我们自己来控制了，因为 OpenGL ES 只负责渲染图形，不负责保存图形。这时候，我们就需要使用 GLES30.glReadPixels() 方法来从 GPU 中拷贝数据到 CPU，然后转换为 Bitmap 或 MediaCodec 等格式，并使用 MediaStore API 或其他方式来保存到磁盘中。</p><h2 id="12-Q-我这里主要是考虑App的性能问题。我现在又有一个疑问，在Android上是有硬件编码的，如果我使用opengles对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么通过opengles处理后的数据是直接从GPU交给硬件编码器呢，还是先拷贝到CPU-再经CPU拷贝到GPU然后编码呢？"><a href="#12-Q-我这里主要是考虑App的性能问题。我现在又有一个疑问，在Android上是有硬件编码的，如果我使用opengles对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么通过opengles处理后的数据是直接从GPU交给硬件编码器呢，还是先拷贝到CPU-再经CPU拷贝到GPU然后编码呢？" class="headerlink" title="12. Q:我这里主要是考虑App的性能问题。我现在又有一个疑问，在Android上是有硬件编码的，如果我使用opengles对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么通过opengles处理后的数据是直接从GPU交给硬件编码器呢，还是先拷贝到CPU,再经CPU拷贝到GPU然后编码呢？"></a>12. Q:我这里主要是考虑App的性能问题。我现在又有一个疑问，在Android上是有硬件编码的，如果我使用opengles对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么通过opengles处理后的数据是直接从GPU交给硬件编码器呢，还是先拷贝到CPU,再经CPU拷贝到GPU然后编码呢？</h2><p>A:如果你使用 OpenGL ES 对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么是否可以直接从 GPU 交给硬件编码器，还是先拷贝到 CPU，再经 CPU 拷贝到 GPU 然后编码，这个问题的答案可能取决于你使用的硬件编码器的类型和接口。一般来说，有两种类型的硬件编码器：一种是集成在 GPU 中的，一种是独立于 GPU 的。如果你使用的是集成在 GPU 中的硬件编码器，那么你可能可以直接从 GPU 交给硬件编码器，或者只需要很少的数据拷贝。如果你使用的是独立于 GPU 的硬件编码器，那么你可能需要先拷贝到 CPU，再经 CPU 拷贝到 GPU 然后编码，或者使用一些特殊的接口来减少数据拷贝。例如，在 Android 平台上，你可以使用 MediaCodec API 来访问硬件编码器，并使用 Surface 对象作为输入和输出¹。这样，你就可以将 OpenGL ES 处理后的图像或视频直接渲染到一个 Surface 对象上，并将这个 Surface 对象传递给 MediaCodec 的 createInputSurface() 方法来创建一个输入 Surface 对象²。然后，你可以将 MediaCodec 的输出 Surface 对象传递给 MediaMuxer 的 addTrack() 方法来创建一个输出 Surface 对象³。这样，你就可以利用 Surface 对象之间的共享内存机制来减少数据拷贝，并使用硬件编码器来编码处理后的图像或视频。</p><p>源: 与必应的对话， 2023&#x2F;5&#x2F;7<br>(1) OpenGL ES | Android Developers. <a target="_blank" rel="noopener" href="https://developer.android.com/develop/ui/views/graphics/opengl/about-opengl">https://developer.android.com/develop/ui/views/graphics/opengl/about-opengl</a>.<br>(2) Implementing OpenGL ES and EGL | Android Open Source Project. <a target="_blank" rel="noopener" href="https://source.android.com/docs/core/graphics/implement-opengl-es">https://source.android.com/docs/core/graphics/implement-opengl-es</a>.<br>(3) Graphics | Android Open Source Project. <a target="_blank" rel="noopener" href="https://source.android.com/docs/core/graphics">https://source.android.com/docs/core/graphics</a>.</p><h2 id="13-Q-根据你的提示，那么android系统下的硬件编码通常是GPU硬件编码器呢还是CPU硬件编码器呢？"><a href="#13-Q-根据你的提示，那么android系统下的硬件编码通常是GPU硬件编码器呢还是CPU硬件编码器呢？" class="headerlink" title="13. Q: 根据你的提示，那么android系统下的硬件编码通常是GPU硬件编码器呢还是CPU硬件编码器呢？"></a>13. Q: 根据你的提示，那么android系统下的硬件编码通常是GPU硬件编码器呢还是CPU硬件编码器呢？</h2><p>A: Android 系统下的硬件编码器的类型可能取决于你使用的设备和芯片的不同，有些设备可能使用集成在 GPU 中的硬件编码器，有些设备可能使用独立于 GPU 的硬件编码器。你可以通过 MediaCodec API 来查询可用的硬件编码器的信息，例如名称、类型、功能等⁴。你也可以通过 MediaCodecInfo.CodecCapabilities 类来获取硬件编码器的能力，例如支持的分辨率、帧率、码率等⁵。这样，你就可以根据你的需求选择合适的硬件编码器来编码处理后的图像或视频。</p><p>源: 与必应的对话， 2023&#x2F;5&#x2F;7<br>(1) Supported media formats | Android Developers. <a target="_blank" rel="noopener" href="https://developer.android.com/guide/topics/media/media-formats">https://developer.android.com/guide/topics/media/media-formats</a>.<br>(2) How to add custom hardware codec to Android Framework? - Bits &amp; Qubits. <a target="_blank" rel="noopener" href="http://gopinaths.gitlab.io/post/custom_hardware_codec_in_android/">http://gopinaths.gitlab.io/post/custom_hardware_codec_in_android/</a>.<br>(3) Android Developers Blog: Android GPU Compute Going Forward. <a target="_blank" rel="noopener" href="https://android-developers.googleblog.com/2021/04/android-gpu-compute-going-forward.html">https://android-developers.googleblog.com/2021/04/android-gpu-compute-going-forward.html</a>.<br>(4) Why processor is “better” for encoding than GPU? - video. <a target="_blank" rel="noopener" href="https://video.stackexchange.com/questions/14656/why-processor-is-better-for-encoding-than-gpu">https://video.stackexchange.com/questions/14656/why-processor-is-better-for-encoding-than-gpu</a>.<br>(5) Implementing Hardware Composer HAL | Android Open Source Project. <a target="_blank" rel="noopener" href="https://source.android.com/docs/core/graphics/implement-hwc">https://source.android.com/docs/core/graphics/implement-hwc</a>.<br>(6) HEIF Imaging | Android Open Source Project. <a target="_blank" rel="noopener" href="https://source.android.com/docs/core/camera/heif">https://source.android.com/docs/core/camera/heif</a>.<br>(7) Android: Check HW Encoder or SW Encoder used in MediaRecorder. <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/50055554/android-check-hw-encoder-or-sw-encoder-used-in-mediarecorder">https://stackoverflow.com/questions/50055554/android-check-hw-encoder-or-sw-encoder-used-in-mediarecorder</a>.</p><h2 id="14-Q-如何通过MediaCodec-API-来查询硬件编码器是-GPU类型-还是CPU类型呢？"><a href="#14-Q-如何通过MediaCodec-API-来查询硬件编码器是-GPU类型-还是CPU类型呢？" class="headerlink" title="14. Q: 如何通过MediaCodec API 来查询硬件编码器是 GPU类型 还是CPU类型呢？"></a>14. Q: 如何通过MediaCodec API 来查询硬件编码器是 GPU类型 还是CPU类型呢？</h2><p>通过 MediaCodec API 来查询硬件编码器是 GPU 类型还是 CPU 类型的方法可能不是很明确，因为 MediaCodec API 并没有提供一个直接的接口来判断这个问题。但是，你可以通过一些间接的方式来推测硬件编码器的类型，例如：</p><ul><li>你可以通过 MediaCodecList.getCodecInfos() 方法来获取所有可用的编码器的信息，然后通过 MediaCodecInfo.getName() 方法来获取编码器的名称。一般来说，硬件编码器的名称会包含一些厂商或芯片的标识，例如 OMX.qcom.video.encoder.h264 表示一个由高通提供的 H.264 硬件编码器，OMX.Intel.hw_vd.h264 表示一个由英特尔提供的 H.264 硬件编码器。你可以根据这些标识来推测硬件编码器是集成在 GPU 中的还是独立于 GPU 的。</li><li>你可以通过 MediaCodecInfo.getCapabilitiesForType() 方法来获取硬件编码器的能力，然后通过 MediaCodecInfo.CodecCapabilities.getVideoCapabilities() 方法来获取视频相关的能力。你可以通过 MediaCodecInfo.VideoCapabilities.getSupportedWidths() 和 getSupportedHeights() 方法来获取硬件编码器支持的分辨率范围。一般来说，集成在 GPU 中的硬件编码器会支持更高的分辨率，例如 4K 或 8K，而独立于 GPU 的硬件编码器可能只支持 1080p 或更低的分辨率。你可以根据这些能力来推测硬件编码器的类型。</li></ul><p>这些方法可能不是很准确，因为不同的设备和芯片可能有不同的实现和命名方式，所以你可能需要根据具体的情况来判断。</p></div><div class="reward-container"><div></div> <button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="https://cdn.avdancedu.com/image/next/wechat.jpeg" alt="音视跳动 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="https://cdn.avdancedu.com/image/next/alipay.jpeg" alt="音视跳动 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> 音视跳动-李超 [avdance@163.com]</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://blog.avdancedu.com/0/" title="">https://blog.avdancedu.com/0/</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="https://cdn.avdancedu.com/image/next/WeChat.jpeg"><span class="icon"><i class="fa fa-wechat"></i></span> <span class="label">WeChat</span></a></div><div class="social-item"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i></span> <span class="label">RSS</span></a></div></div></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/bdaf6202/" rel="prev" title="C++20新特性之module"><i class="fa fa-chevron-left"></i> C++20新特性之module</a></div><div class="post-nav-item"><a href="/0/" rel="next" title=""><i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Q-android-camerax-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-opengles"><span class="nav-number">1.</span> <span class="nav-text">1. Q: android camerax 如何使用 opengles ?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Q-%E5%8F%AF%E4%BB%A5%E4%B8%BE%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E4%BE%8B%E5%AD%90%E5%90%97%EF%BC%9F"><span class="nav-number">2.</span> <span class="nav-text">2. Q: 可以举个具体的例子吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Q-%E5%9C%A8%E4%B8%8A%E8%BF%B0%E4%BB%A3%E7%A0%81%E4%B8%AD%EF%BC%8C%E6%98%AF%E5%A6%82%E4%BD%95%E5%B0%86preview%E4%B8%8E-GLSurfaceView%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">3.</span> <span class="nav-text">3. Q: 在上述代码中，是如何将preview与 GLSurfaceView联系起来的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Q-%E8%83%BD%E6%8A%8A%E7%9B%B8%E6%9C%BA%E8%8E%B7%E5%8F%96%E5%9B%BE%E5%83%8F%E5%90%8E%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC%E5%88%B0preview%E7%9A%84%E5%AE%8C%E6%95%B4%E8%BF%87%E7%A8%8B%E8%AF%A6%E7%BB%86%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%90%97%EF%BC%9F"><span class="nav-number">4.</span> <span class="nav-text">4. Q: 能把相机获取图像后，数据流转到preview的完整过程详细描述一下吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Q-%E8%BF%99%E9%87%8C%E6%9C%89%E5%87%A0%E4%B8%AA%E7%82%B9%E6%88%91%E8%A7%89%E5%BE%97%E6%8C%BA%E5%A5%87%E6%80%AA%E3%80%82%E5%92%B1%E4%BB%AC%E6%9D%A5%E4%B8%80%E9%A1%B9%E9%A1%B9%E8%AE%A8%E8%AE%BA%E3%80%82%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AFpreview-%E6%98%AFcamerax%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%8E%A5%E6%94%B6%E8%80%85%E5%90%97%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%98%AF%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%9C%A8xml%E4%B8%AD%E6%98%AF%E4%B8%8D%E6%98%AF%E8%A6%81%E5%AE%9A%E4%B9%89%E8%BF%99%E4%B8%AApreview%E6%8E%A7%E4%BB%B6%E5%91%A2%EF%BC%9F"><span class="nav-number">5.</span> <span class="nav-text">5. Q: 这里有几个点我觉得挺奇怪。咱们来一项项讨论。第一个是preview 是camerax的第一个接收者吗？如果是的话，在xml中是不是要定义这个preview控件呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Q-%E7%AC%AC%E4%BA%8C%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">6.</span> <span class="nav-text">6. Q: 第二个问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Q-ok-%E6%8C%89%E7%85%A7%E4%BD%A0%E7%9A%84%E8%AF%B4%E6%B3%95%EF%BC%8CCamerax%E8%8E%B7%E5%8F%96%E5%88%B0%E5%9B%BE%E5%83%8F%E5%90%8E%EF%BC%8C%E4%BC%9A%E5%B0%86%E6%95%B0%E6%8D%AE%E8%BD%AC%E4%BA%A4%E7%BB%99preview%EF%BC%8C%E6%88%91%E4%BB%AC%E5%86%8D%E4%B8%BApreview%E6%8F%90%E4%BE%9B%E4%B8%80%E4%B8%8BSurfaceProvider-%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87SurfaceProvider%E6%8B%BF%E5%88%B0%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E9%80%9A%E8%BF%87GLSurfaceView%E5%92%8C-Render%E5%B0%B1%E5%8F%AF%E4%BB%A5%E5%AF%B9%E6%95%B0%E6%8D%AE%E4%BD%BF%E7%94%A8opengles%E8%BF%9B%E8%A1%8C%E6%B8%B2%E6%9F%93%EF%BC%8C%E5%B9%B6%E6%9C%80%E7%BB%88%E5%B0%86%E6%B8%B2%E6%9F%93%E7%9A%84%E7%BB%93%E6%9E%9C%E5%9C%A8%E5%B1%8F%E5%B9%95%E4%B8%8A%E6%98%BE%E7%A4%BA%E5%87%BA%E6%9D%A5%E4%BA%86%E6%98%AF%E5%90%A7%EF%BC%9F"><span class="nav-number">7.</span> <span class="nav-text">7. Q: ok, 按照你的说法，Camerax获取到图像后，会将数据转交给preview，我们再为preview提供一下SurfaceProvider, 这样就可以通过SurfaceProvider拿到图像数据，然后再通过GLSurfaceView和 Render就可以对数据使用opengles进行渲染，并最终将渲染的结果在屏幕上显示出来了是吧？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Q-OK-%E9%82%A3%E5%A6%82%E6%9E%9C%E6%83%B3%E5%B0%86%E5%A4%84%E7%90%86%E5%90%8E%E7%9A%84%E5%9B%BE%E7%89%87%E4%BF%9D%E5%AD%98%E8%B5%B7%E6%9D%A5%EF%BC%8C%E8%BF%98%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%90%97%EF%BC%9F%E5%BA%94%E8%AF%A5%E4%B8%8D%E5%8F%AF%E4%BB%A5%E4%BA%86%E5%90%A7%EF%BC%9F"><span class="nav-number">8.</span> <span class="nav-text">8. Q:OK, 那如果想将处理后的图片保存起来，还可以通过 cameraProvider.bindToLifecycle( this, cameraSelector, preview, imageCapture) 这种方式吗？应该不可以了吧？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Q-GLES30-glReadPixels-%E6%96%B9%E6%B3%95%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E5%B0%86GPU%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D%E5%88%B0CPU%E4%B8%AD%E5%90%97%EF%BC%9F"><span class="nav-number">9.</span> <span class="nav-text">9. Q: GLES30.glReadPixels() 方法的作用是将GPU的数据拷贝到CPU中吗？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Q-%E9%82%A3%E9%80%9A%E8%BF%87-cameraProvider-bindToLifecycle-this-cameraSelector-preview-imageCapture-%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%AD%98%E5%82%A8%E5%9B%BE%E7%89%87%E6%97%B6%EF%BC%8C%E5%AE%83%E5%BA%95%E5%B1%82%E6%98%AF%E5%90%A6%E4%B9%9F%E4%BC%9A%E5%B0%86-GPU%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D%E5%88%B0CPU%E5%86%8D%E7%BC%96%E8%BF%87%E7%BC%96%E7%A0%81%E4%BF%9D%E5%AD%98%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%AD%E5%91%A2%EF%BC%9F"><span class="nav-number">10.</span> <span class="nav-text">10. Q: 那通过 cameraProvider.bindToLifecycle(this, cameraSelector, preview, imageCapture) 这种方式存储图片时，它底层是否也会将 GPU的数据拷贝到CPU再编过编码保存到磁盘中呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-Q-%E6%88%91%E7%9A%84%E6%84%8F%E6%80%9D%E6%98%AF%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%B6%89%E5%8F%8A%E5%88%B0%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87%EF%BC%8C%E6%88%96%E4%BF%9D%E5%AD%98%E8%A7%86%E9%A2%91%EF%BC%8C%E6%98%AF%E5%90%A6%E5%B0%B1%E7%BB%95%E4%B8%8D%E5%BC%80%E4%BB%8EGPU%E4%B8%AD%E6%8B%B7%E8%B4%9D%E6%95%B0%E6%8D%AE%E5%88%B0CPU-%E7%84%B6%E5%90%8E%E7%BC%96%E7%A0%81%EF%BC%8C%E6%9C%80%E5%90%8E%E4%BF%9D%E5%AD%98%E5%88%B0%E7%A3%81%E7%9B%98%E4%B8%AD%E8%BF%99%E6%A0%B7%E4%B8%80%E4%B8%AA%E6%B5%81%E7%A8%8B%E5%91%A2%EF%BC%9F"><span class="nav-number">11.</span> <span class="nav-text">11. Q: 我的意思是，如果涉及到保存图片，或保存视频，是否就绕不开从GPU中拷贝数据到CPU,然后编码，最后保存到磁盘中这样一个流程呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-Q-%E6%88%91%E8%BF%99%E9%87%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E8%80%83%E8%99%91App%E7%9A%84%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E3%80%82%E6%88%91%E7%8E%B0%E5%9C%A8%E5%8F%88%E6%9C%89%E4%B8%80%E4%B8%AA%E7%96%91%E9%97%AE%EF%BC%8C%E5%9C%A8Android%E4%B8%8A%E6%98%AF%E6%9C%89%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E7%9A%84%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BD%BF%E7%94%A8opengles%E5%AF%B9%E5%9B%BE%E5%83%8F%E6%88%96%E8%A7%86%E9%A2%91%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%A4%84%E7%90%86%EF%BC%8C%E7%84%B6%E5%90%8E%E6%83%B3%E5%AF%B9%E5%A4%84%E7%90%86%E5%90%8E%E7%9A%84%E5%9B%BE%E5%83%8F%E6%88%96%E8%A7%86%E9%A2%91%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A0%81%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%83%B3%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%EF%BC%8C%E9%82%A3%E4%B9%88%E9%80%9A%E8%BF%87opengles%E5%A4%84%E7%90%86%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%AF%E7%9B%B4%E6%8E%A5%E4%BB%8EGPU%E4%BA%A4%E7%BB%99%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E5%99%A8%E5%91%A2%EF%BC%8C%E8%BF%98%E6%98%AF%E5%85%88%E6%8B%B7%E8%B4%9D%E5%88%B0CPU-%E5%86%8D%E7%BB%8FCPU%E6%8B%B7%E8%B4%9D%E5%88%B0GPU%E7%84%B6%E5%90%8E%E7%BC%96%E7%A0%81%E5%91%A2%EF%BC%9F"><span class="nav-number">12.</span> <span class="nav-text">12. Q:我这里主要是考虑App的性能问题。我现在又有一个疑问，在Android上是有硬件编码的，如果我使用opengles对图像或视频进行了处理，然后想对处理后的图像或视频进行编码，并且想使用硬件编码，那么通过opengles处理后的数据是直接从GPU交给硬件编码器呢，还是先拷贝到CPU,再经CPU拷贝到GPU然后编码呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#13-Q-%E6%A0%B9%E6%8D%AE%E4%BD%A0%E7%9A%84%E6%8F%90%E7%A4%BA%EF%BC%8C%E9%82%A3%E4%B9%88android%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E9%80%9A%E5%B8%B8%E6%98%AFGPU%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E5%99%A8%E5%91%A2%E8%BF%98%E6%98%AFCPU%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E5%99%A8%E5%91%A2%EF%BC%9F"><span class="nav-number">13.</span> <span class="nav-text">13. Q: 根据你的提示，那么android系统下的硬件编码通常是GPU硬件编码器呢还是CPU硬件编码器呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#14-Q-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87MediaCodec-API-%E6%9D%A5%E6%9F%A5%E8%AF%A2%E7%A1%AC%E4%BB%B6%E7%BC%96%E7%A0%81%E5%99%A8%E6%98%AF-GPU%E7%B1%BB%E5%9E%8B-%E8%BF%98%E6%98%AFCPU%E7%B1%BB%E5%9E%8B%E5%91%A2%EF%BC%9F"><span class="nav-number">14.</span> <span class="nav-text">14. Q: 如何通过MediaCodec API 来查询硬件编码器是 GPU类型 还是CPU类型呢？</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="https://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="音视跳动" src="/images/avdancelogo.jpg"><p class="site-author-name" itemprop="name">音视跳动</p><div class="site-description" itemprop="description">传输最前沿的科技知识，学习音视频的圣地！ffmpeg, webrtc, H264, AAC</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">71</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">28</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">46</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/avdance" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;avdance" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/garrylea/posts" title="ZhiHu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;garrylea&#x2F;posts" rel="noopener" target="_blank"><i class="fa fa-fw fa-zhihu"></i> ZhiHu</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://www.beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备19056322号-1</a> <img src="/images/beianico.png" style="display:inline-block"><a href="https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=11011102001366" rel="noopener" target="_blank">京公网安备11011102001366号</a></div><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">李超</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span title="站点总字数">330k</span></div><script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"Cu6h7B3RnNt3uEMrRWpVlIU6-gzGzoHsz","app_key":"5dKv9XFD2w3gjJnb0xnWIIWz","server_url":"https://leancloud.cn","security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        //if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/lozad.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("Cu6h7B3RnNt3uEMrRWpVlIU6-gzGzoHsz","5dKv9XFD2w3gjJnb0xnWIIWz")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.min.css"><script src="/lib/needsharebutton/needsharebutton.min.js"></script><script>flOptions={iconStyle:"box",boxForm:"horizontal",position:"topRight",networks:"Weibo,Wechat,QQZone"},new needShareButton("#needsharebutton-float",flOptions)</script><script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('/lib/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Cu6h7B3RnNt3uEMrRWpVlIU6-gzGzoHsz',
      appKey     : '5dKv9XFD2w3gjJnb0xnWIIWz',
      placeholder: "畅所欲言？",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script></body></html>